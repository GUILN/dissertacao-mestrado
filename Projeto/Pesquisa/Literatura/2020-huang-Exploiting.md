# 2020-huang-Exploiting

- [Exploiting Cross-session Information for Session-based Recommendation with Graph Neural Networks](https://www.semanticscholar.org/paper/Exploiting-Cross-session-Information-for-with-Graph-Qiu-Huang/9f20be759484ab786af69e3e5f450f50d33e0480)
- **Tags:** #GNN #cross-session #attention
- **Published At:** [[ACM Transactions on Information Systems]]

## Main Information

| Model        | Metric                | Dataset                         | Citations | Date       | Results | Objective |
| ------------ | --------------------- | ------------------------------- | --------- | ---------- | ------- | --------- |
| FGNN and BCS | #p_at_k #MRR_at_k #RK | [[Diginetica]]<br>[[Yoochoose]] | 112       | 21/05/2020 |         |           |


## Method

**Description:** 
The method takes the current anonymous session $S$ and cross session information as input and predict the next Item: $V_{s, n+1}$ 

- BCS (Broad Connected Session) Graph:
	- The session sequence is transformed into a weighted directed graph $G_s = (V_s, E_s), G_s \in G$ where $G$ is the set of all session graphs
	- The edge set $E_s$ represents all directed weighted edges where the weight of the edge is given by the frequency of the occurrence of that edge within the session.
	- For the `self-attention` they used #WGAT 
	- If a node does not contain a `self loop`, a `self loop` is added to that node due to authors saying that it is common for the a user to click more than once in a product consecutively.
	- After converting the session $S$ into a graph $G_s$ the final embedding of the session is based on the calculation of this session graph $G_s$. 
	- Construction:
		- Every single session is converted into a graph $G_s$ 
		- A global graph $G_{full}$ is generated by concatenating all $n$ edges set $\sum_{s = 0}^{n}{E_{s}}$ (it is possible to be seem in the second image)
- FGNN - Full Graph Neural Network
	- It uses #WGAT - Weighted Graph Attention Layer
	- It uses Mask-Readout function to aggregate item embeddings being "distracted" by #cross-session information
- WGAT:
	- After obtaining a `session graph` a GNN is used to learn embeddings
	- Although a #GAT or #GCN can be used to learn the graph embeddings, it does not includes information of weight and direction, therefore #WGAT is proposed by this paper
	- The input of WGAT is the node's features: $x = \{x_1, x_2, x_3, x_{n - 1}\}, x_i \in \mathbb{R}^{d}$, where $n$ is the number of nodes in the graph and $d$ is the dimension of embedding of $x_{i}$. After applying WGAT a new set of nodes will be generated:  $x' = \{x_1', x_2', x_3', x_{n - 1}'\}, x_i' \in \mathbb{R'}^{d'}$
	- The input vectors $x_{i}^{0}$ of the first WGAT layer are generated from an embedding layer whose inputs is the #one-hot-encoding of items: $x_{i}^{0} = Embed(v_i)$
	- `self-attention mechanism` for every node $i$ is used to aggregate information from its neighboring nodes $N(i)$  - Every neighbor is taken in consideration given the size of a session tends not to be huge, the `self-attention coeficient` $e_{ij}$ is used to determine how importantly the node *j* will influence the node $i$ and it is calculated based on $x_{i}, x_{j}$ and $w_{ij}$, $e_{ij} = Att(W_{x_{i}}, W_{x_{j}}, w_{ij})$, where $Att$ is a mapping and $W$ is a shared parameter which performs linear mapping across all nodes
		- They restrict the range of the attention within the first order neighbor of node $i$ 

**What it proposes:**
Tries to address 2 problems:
1. Model complicated item dependency relationships to improve the item and session #embedding
2. Exploit and incorporate #cross-session information into the current session in the setting of anonymous session

**Method Overview:**
![[Screenshot 2024-03-22 at 16.48.10.png]]

![[Screenshot 2024-03-28 at 11.17.53.png]]

## Experiments
#p_at_20 #MRR_at_20 - #Diginetica #Yoochoose 

## Metrics
#p_at_k #MRR_at_k

## Results

|            | **Diginetica** | **Yoochose** |
| ---------- | -------------- | ------------ |
| **R@20**   | 51.67          | 71.75        |
| **MRR@20** | 18.74          | 32.45        |

## Conclusions

- The FGNN model utilizes a weighted graph attentional layer (WGAT) and a Readout function to learn representative features and the order of item dependency, respectively
- It is compared to non-GNN methods by the authors (which does not have much value)

## Free Notes

**Introduction:**
- Explains and shows the correlation (using Pearsons correlation) between sessions with at least one item in common is high (mean of 0.43), although it does not shares how the Pearson's coefficient was calculated
